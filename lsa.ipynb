{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boring-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worldwide-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naked-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The amount of polution is increasing day by day',\n",
       " 'The concert was just great',\n",
       " 'I love to see Gordon Ramsay cook',\n",
       " 'Google is introducing a new technology',\n",
       " 'AI Robots are examples of great technology present today',\n",
       " 'All of us were singing in the concert',\n",
       " 'We have launch campaigns to stop pollution and global warming']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [\"The amount of polution is increasing day by day\",\n",
    "           \"The concert was just great\",\n",
    "           \"I love to see Gordon Ramsay cook\",\n",
    "           \"Google is introducing a new technology\",\n",
    "           \"AI Robots are examples of great technology present today\",\n",
    "           \"All of us were singing in the concert\",\n",
    "           \"We have launch campaigns to stop pollution and global warming\"]\n",
    "dataset\n",
    "#Note this dataset is already preprocessed but we are going to convert all capital letters to small letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dependent-bidding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the amount of polution is increasing day by day',\n",
       " 'the concert was just great',\n",
       " 'i love to see gordon ramsay cook',\n",
       " 'google is introducing a new technology',\n",
       " 'ai robots are examples of great technology present today',\n",
       " 'all of us were singing in the concert',\n",
       " 'we have launch campaigns to stop pollution and global warming']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset preprocessing\n",
    "dataset=[line.lower() for line in dataset]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adolescent-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.3211483974289089\n",
      "  (0, 9)\t0.6422967948578178\n",
      "  (0, 17)\t0.3211483974289089\n",
      "  (0, 19)\t0.2665807498646048\n",
      "  (0, 26)\t0.3211483974289089\n",
      "  (0, 24)\t0.2278643877752444\n",
      "  (0, 2)\t0.3211483974289089\n",
      "  (0, 34)\t0.2278643877752444\n"
     ]
    }
   ],
   "source": [
    "#now we are going to create a BOW model from this dataset which can be a TF-IDF model or binary BOW model\n",
    "#note there are already classes availaible binary BOW model from text and one of such classes is TfidfVectorizer\n",
    "#which can create TF-IDF model from this list of strings.\n",
    "#So why are we using this TfidfVectorizer when we can create our own model?\n",
    "#the reason is that this Vectorizer not only converts the dataset into TF-IDF modlel it also has additional features\n",
    "#as well like you can get the different feature names. So we can know which column corresponds to which word and so on\n",
    "#but you can also use your own tfidf model\n",
    "\n",
    "vectorizer = TfidfVectorizer()  #creating object\n",
    "X = vectorizer.fit_transform(dataset) #what this will do is it will convert the dataset into a TF-IDF model\n",
    "\n",
    "print(X[0])\n",
    "#here we have specified 0 which means we are checking the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elect-footwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=4, n_iter=100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we are going to decompose this matrix into those 3 matrices and we are going to do that using another class\n",
    "#TruncatedSVD\n",
    "\n",
    "lsa = TruncatedSVD(n_components = 4, n_iter = 100)#here n_components is no. of concepts you want to find from the data\n",
    "#n_iter is the no. of iterations so when you are decomposing a matrix in the first iteration it is does something ,\n",
    "#in the second iteration it tries to modify it and make it better and so on. So it will do 100 iteraions to properly\n",
    "#decompose that matix into 3 matrices and you can even pass here 300 (the higher the better)\n",
    "\n",
    "lsa.fit(X) #fitting the X and creating the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fundamental-corps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.24191973e-01,  1.78240252e-01,  1.14460798e-01, -7.98450857e-18,\n",
       "        1.24191973e-01,  1.14460798e-01, -7.98450857e-18,  3.44988739e-01,\n",
       "       -4.62382034e-17,  2.28921595e-01,  1.24191973e-01, -7.98450857e-18,\n",
       "        9.72770950e-02, -4.62382034e-17,  3.00124026e-01, -7.98450857e-18,\n",
       "        1.78240252e-01,  1.14460798e-01,  9.72770950e-02,  1.75760635e-01,\n",
       "        2.37365829e-01, -7.98450857e-18, -4.62382034e-17,  9.72770950e-02,\n",
       "        2.95798061e-01, -7.98450857e-18,  1.14460798e-01,  1.24191973e-01,\n",
       "       -4.62382034e-17,  1.24191973e-01, -4.62382034e-17,  1.78240252e-01,\n",
       "       -7.98450857e-18,  1.83838346e-01,  3.76098295e-01, -4.50095076e-17,\n",
       "        1.24191973e-01,  1.78240252e-01, -7.98450857e-18,  2.37365829e-01,\n",
       "       -7.98450857e-18,  1.78240252e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets find out first row of the model\n",
    "row1 = lsa.components_[0]\n",
    "row1\n",
    "#when we were talking about SVD we decomposed the matrix into 3 matrices but using this truncated SVD you can get only\n",
    "#that last one which is V transpose (r*n) and those n columns are n different word and r rows are the concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "humanitarian-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'all', 'amount', 'and', 'are', 'by', 'campaigns', 'concert', 'cook', 'day', 'examples', 'global', 'google', 'gordon', 'great', 'have', 'in', 'increasing', 'introducing', 'is', 'just', 'launch', 'love', 'new', 'of', 'pollution', 'polution', 'present', 'ramsay', 'robots', 'see', 'singing', 'stop', 'technology', 'the', 'to', 'today', 'us', 'warming', 'was', 'we', 'were']\n"
     ]
    }
   ],
   "source": [
    "#here when we have written lsa.components_[0] it means that i am returning the first row of the V transpose matrix\n",
    "#some words are in this concepts which have very high value and some words that are not in this concepts which have \n",
    "#very low value here. So now have the concepts value for all the different words. Now what we are going to do  We is \n",
    "#lets display this whole thing properly. We are going to display that corresponding to each concept which are the most \n",
    "#importnt words. In case of google corresponding to each concept google has a list of keywords and those are the most\n",
    "#important keywords that occur in that specific concept.\n",
    "\n",
    "terms = vectorizer.get_feature_names() #using this we can get all the different words that are in the tfidf model\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "outdoor-edmonton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "planned-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept  0 :\n",
      "('the', 0.3760982952926376)\n",
      "('concert', 0.344988739233066)\n",
      "('great', 0.3001240258948742)\n",
      "('of', 0.2957980609526667)\n",
      "('just', 0.23736582929791256)\n",
      "('was', 0.23736582929791256)\n",
      "('day', 0.22892159541504534)\n",
      "('technology', 0.1838383456741343)\n",
      "('all', 0.17824025175628944)\n",
      "('in', 0.17824025175628944)\n",
      "\n",
      "Concept  1 :\n",
      "('to', 0.41578844396700687)\n",
      "('cook', 0.28359165793510716)\n",
      "('gordon', 0.28359165793510716)\n",
      "('love', 0.28359165793510716)\n",
      "('ramsay', 0.28359165793510716)\n",
      "('see', 0.28359165793510716)\n",
      "('and', 0.21730644711292482)\n",
      "('campaigns', 0.21730644711292482)\n",
      "('global', 0.21730644711292482)\n",
      "('have', 0.21730644711292482)\n",
      "\n",
      "Concept  2 :\n",
      "('technology', 0.3779180676714393)\n",
      "('is', 0.3419614380631994)\n",
      "('google', 0.34139694419097494)\n",
      "('introducing', 0.34139694419097494)\n",
      "('new', 0.34139694419097494)\n",
      "('day', 0.14112432680994852)\n",
      "('are', 0.11387892195372905)\n",
      "('examples', 0.11387892195372905)\n",
      "('present', 0.11387892195372905)\n",
      "('robots', 0.11387892195372905)\n",
      "\n",
      "Concept  3 :\n",
      "('day', 0.4654267679041107)\n",
      "('by', 0.23271338395205535)\n",
      "('increasing', 0.23271338395205535)\n",
      "('polution', 0.23271338395205535)\n",
      "('amount', 0.23271338395205532)\n",
      "('is', 0.21264455202449883)\n",
      "('the', 0.127242131806944)\n",
      "('in', 0.05644664752726603)\n",
      "('singing', 0.05644664752726603)\n",
      "('us', 0.05644664752726603)\n"
     ]
    }
   ],
   "source": [
    "#note here we have all the terms i.e., 42 terms that is the reason why we had 42 in row1\n",
    "#now the main job\n",
    "\n",
    "for i,comp in enumerate(lsa.components_): #looping through all the different rows of v transpose matrix\n",
    "    #here this enumerate each time will return the index and the row\n",
    "    componentTerms = zip(terms,comp)\n",
    "    #note in the terms we have 42 different words so here we wil generate a new list where the new list will be a list\n",
    "    #of tuples and each tuple will contain the word and its corresponding concept value\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "    #here we are sorting the whole component terms so the words that have the highest component value would be at the \n",
    "    #top and so on. lambda x:x[1] means x corresponds to the each of tuples stored in the whole list of component\n",
    "    #terms and x[1] is the concept value so what we are telling using this is that you are going to sort this whole\n",
    "    #component list based on the concept value of all the different words. Now if we used x:x[0] instead of this we\n",
    "    #would sort it using the word and reverse = True means sorting in decending order\n",
    "    sortedTerms = sortedTerms[:10] #it will select only 10 most important terms in a specific concept\n",
    "    print(\"\\nConcept \",i,\":\")\n",
    "    for term in sortedTerms:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "august-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it might not be very accurate but it is pretty good\n",
    "#so now under each concept we have all the different keywords along with their probabilities as well now we can use \n",
    "#this to find out which document is from which concept "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secure-webmaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Concept 0': [('the', 0.3760982952926376), ('concert', 0.344988739233066), ('great', 0.3001240258948742), ('of', 0.2957980609526667), ('just', 0.23736582929791256), ('was', 0.23736582929791256), ('day', 0.22892159541504534), ('technology', 0.1838383456741343), ('all', 0.17824025175628944), ('in', 0.17824025175628944)], 'Concept 1': [('to', 0.41578844396700687), ('cook', 0.28359165793510716), ('gordon', 0.28359165793510716), ('love', 0.28359165793510716), ('ramsay', 0.28359165793510716), ('see', 0.28359165793510716), ('and', 0.21730644711292482), ('campaigns', 0.21730644711292482), ('global', 0.21730644711292482), ('have', 0.21730644711292482)], 'Concept 2': [('technology', 0.3779180676714393), ('is', 0.3419614380631994), ('google', 0.34139694419097494), ('introducing', 0.34139694419097494), ('new', 0.34139694419097494), ('day', 0.14112432680994852), ('are', 0.11387892195372905), ('examples', 0.11387892195372905), ('present', 0.11387892195372905), ('robots', 0.11387892195372905)], 'Concept 3': [('day', 0.4654267679041107), ('by', 0.23271338395205535), ('increasing', 0.23271338395205535), ('polution', 0.23271338395205535), ('amount', 0.23271338395205532), ('is', 0.21264455202449883), ('the', 0.127242131806944), ('in', 0.05644664752726603), ('singing', 0.05644664752726603), ('us', 0.05644664752726603)]}\n"
     ]
    }
   ],
   "source": [
    "#now we are going to find out what is the concept for which document\n",
    "#crating a dictionary which will store all the concepts mapped with the list of the different tuples\n",
    "concept_words = {}\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(terms,comp)\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "    sortedTerms = sortedTerms[:10]\n",
    "    concept_words[\"Concept \"+str(i)] = sortedTerms\n",
    "\n",
    "print(concept_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "transsexual-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept 0:\n",
      "1.129739547075395\n",
      "1.4959427190164032\n",
      "0\n",
      "0.1838383456741343\n",
      "0.7797604325216752\n",
      "1.3733655989909492\n",
      "0\n",
      "\n",
      "Concept 1:\n",
      "0\n",
      "0\n",
      "1.8337467336425428\n",
      "0\n",
      "0\n",
      "0\n",
      "1.2850142324187064\n",
      "\n",
      "Concept 2:\n",
      "0.6242100916830964\n",
      "0\n",
      "0\n",
      "1.7440703383075635\n",
      "0.8334337554863556\n",
      "0\n",
      "0\n",
      "\n",
      "Concept 3:\n",
      "2.2015937554478855\n",
      "0.127242131806944\n",
      "0\n",
      "0.21264455202449883\n",
      "0\n",
      "0.29658207438874207\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#now we are going to use it to know which document is from which concept\n",
    "for key in concept_words.keys(): #looping through all of the concepts\n",
    "    sentence_scores = [] #it is going to store the scores for all the different sentences for this specific concept\n",
    "    for sentence in dataset:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        score = 0 #this is going to contain the score for each of the different sentences in the whole dataset for a\n",
    "        #specific context\n",
    "        for word in words:\n",
    "            for word_with_score in concept_words[key]:#looping through list of tuples corresponding to a concept\n",
    "                if word == word_with_score[0]:\n",
    "                    score+=word_with_score[1]\n",
    "        sentence_scores.append(score)\n",
    "    print(\"\\n\"+key+\":\")\n",
    "    for sentence_score in sentence_scores:\n",
    "        print(sentence_score)\n",
    "    \n",
    "#so what did we do here here for each of the sentence in the spescific concept we are tokenizing each of those\n",
    "#sentences into this list of words and also for each sentence we are initializing the score to be 0 and then looping \n",
    "#through all the words in the sentence and inside that we are looping through all the words in the specific concept\n",
    "#and then we are trying to see that the words that specific sentence contains wheather any of those words are in the \n",
    "#keyword list of the concept and if it contains any of the word we are simply adding their scores and after that we\n",
    "#are appending that score to the sentence with scores\n",
    "                \n",
    "#so we can see that in concept 0 is second one (i.e., with hoghest score) which is \"the concert was just great\"\n",
    "#also note in second concept we have a misclassification of the last sentence which shows it is not very accurate\n",
    "#but still gets the job done now the reason for misclassification is that we have very less amount of data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
